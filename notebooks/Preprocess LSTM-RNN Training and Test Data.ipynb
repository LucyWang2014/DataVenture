{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Lucy/anaconda/envs/capstone/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import pickle\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import itertools\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import unicodedata\n",
    "import sys\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mypath = '/Users/Lucy/Google Drive/MSDS/2016Fall/DSGA1006/Data'\n",
    "f = open(mypath + '/lstm-rnn/lstm_data.pickle','rb')\n",
    "X_train, X_test, y_train, y_test = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_description = X_train.clean_description.copy()\n",
    "test_description = X_test.clean_description.copy()\n",
    "#train_description = [word for word in train_description if word not in stopwords.words('english')]\n",
    "#test_description = [word for word in test_description if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 5000\n",
    "unknown_token = \"UNKNOWNTOKEN\"\n",
    "sentence_start_token = \"SENTENCESTART\"\n",
    "sentence_end_token = \"SENTENCEEND\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split full descriptions into sentences\n",
    "sentences = itertools.chain(*[[nltk.sent_tokenize(x.decode('utf-8').lower())] for x in train_description])\n",
    "mod_sentences = []\n",
    "for desc in sentences:\n",
    "    full_desc = \"\"\n",
    "    for x in desc:\n",
    "        s = \"%s %s %s \" % (sentence_start_token, x, sentence_end_token)\n",
    "        full_desc = full_desc + s\n",
    "    mod_sentences.append(full_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strip_punctuation(text):\n",
    "    \"\"\"\n",
    "    >>> strip_punctuation(u'something')\n",
    "    u'something'\n",
    "\n",
    "    >>> strip_punctuation(u'something.,:else really')\n",
    "    u'somethingelse really'\n",
    "    \"\"\"\n",
    "    punctutation_cats = set(['Pc', 'Pd', 'Ps', 'Pe', 'Pi', 'Pf', 'Po'])\n",
    "    return ''.join(x for x in text\n",
    "                   if unicodedata.category(x) not in punctutation_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mod_sentences = map(strip_punctuation, mod_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCESTART cytori therapeutics inc is dedicated to developing and delivering innovative cellbased therapies to improve the quality and length of life SENTENCEEND SENTENCESTART we are a global public company engaged in the development and commercialization of stem and regenerative cell therapies for the treatment of cardiovascular disease reconstructive surgery and other serious chronic and life threatening conditions SENTENCEEND \n",
      "\n",
      "SENTENCESTART avva is a personal online application that helps women who have been diagnosed with breast cancer prepare for important medical appointments SENTENCEEND SENTENCESTART avvas vision is to empower and support patients by providing an online tool that serves as a comprehensive home base for breast cancer treatment  one that helps patients address the organizational emotional social and medical components of treatment SENTENCEEND SENTENCESTART avva is a personal advocate that gives patients tools to take a more active role in their own treatment SENTENCEEND SENTENCESTART avvas visit preparation service is the first step on the path to this vision SENTENCEEND SENTENCESTART the founders time spent working closely with patients has led them to focus first and foremost on improving patient experience maintaining an absolute dedication to serving patients above and beyond any other parties in the healthcare system SENTENCEEND SENTENCESTART the evolution of avvas functionality and design has been driven almost exclusively by countless interviews and feedback sessions with breast cancer survivors SENTENCEEND SENTENCESTART the result is a simple intuitive tool that the team at avva believes can have a tremendously positive impact on any patient undergoing breast cancer treatment SENTENCEEND \n",
      "\n",
      "SENTENCESTART wellpath is a new york based startup that delivers an innovative solution to personalized modern wellness SENTENCEEND SENTENCESTART by taking advanced nutritional scientific research available and applying inhouse expertise and proprietary technology wellpath formulates truly customized nutritional solutions with ingredients selected for the specific needs and goals of a customer SENTENCEEND SENTENCESTART by providing ingredients based on substantiated scientific research wellpath aims to empower the consumer and aid them in their personal journey to a healthier life SENTENCEEND SENTENCESTART they firmly believe that one individuals body is not the same as their friends or spouses so they shouldnt take things just because others in their life do SENTENCEEND \n",
      "\n",
      "SENTENCESTART diveling is a company that brings services to the reader based on the context of the page they are on SENTENCEEND SENTENCESTART we make the page more rich and responsive more entertaining SENTENCEEND SENTENCESTART combining annotation with a search engine and social network capabilities we change the web page from a bunch of text with hyperlinks to a dynamic and social page SENTENCEEND SENTENCESTART you can write on the page exchange ideas follow ideas or discover extra information brought by the search engine just by highlighting the text of interest SENTENCEEND SENTENCESTART we bring all the dimensions of what the internet really is to the page you are reading SENTENCEEND SENTENCESTART diveling brings improved productivity contextual collaboration and intuitive emarketing capabilities SENTENCEEND \n",
      "\n",
      "SENTENCESTART handsfree networks is a global provider of software and services that automate the resolution of technology problems SENTENCEEND SENTENCESTART as a result technology critical to realtime businesses  as well as technology and related services that consumers have come to rely upon  work as planned SENTENCEEND SENTENCESTART handsfree networks solutions all take advantage of patented functionality in its support automation software platform which provides the ability to precisely identify diagnose and resolve technical problems via proactive self or assisted service solutions SENTENCEEND \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in mod_sentences[:5]:\n",
    "    print i + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in mod_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCESTART\n",
      "UNKNOWNTOKEN\n",
      "is\n",
      "a\n",
      "company\n",
      "that\n",
      "brings\n",
      "services\n",
      "to\n",
      "the\n",
      "reader\n",
      "based\n",
      "on\n",
      "the\n",
      "context\n",
      "of\n",
      "the\n",
      "page\n",
      "they\n",
      "are\n",
      "on\n",
      "SENTENCEEND\n",
      "SENTENCESTART\n",
      "we\n",
      "make\n",
      "the\n",
      "page\n",
      "more\n",
      "rich\n",
      "and\n",
      "responsive\n",
      "more\n",
      "entertaining\n",
      "SENTENCEEND\n",
      "SENTENCESTART\n",
      "combining\n",
      "UNKNOWNTOKEN\n",
      "with\n",
      "a\n",
      "search\n",
      "engine\n",
      "and\n",
      "social\n",
      "network\n",
      "capabilities\n",
      "we\n",
      "change\n",
      "the\n",
      "web\n",
      "page\n",
      "from\n",
      "a\n",
      "UNKNOWNTOKEN\n",
      "of\n",
      "text\n",
      "with\n",
      "UNKNOWNTOKEN\n",
      "to\n",
      "a\n",
      "dynamic\n",
      "and\n",
      "social\n",
      "page\n",
      "SENTENCEEND\n",
      "SENTENCESTART\n",
      "you\n",
      "can\n",
      "write\n",
      "on\n",
      "the\n",
      "page\n",
      "exchange\n",
      "ideas\n",
      "follow\n",
      "ideas\n",
      "or\n",
      "discover\n",
      "extra\n",
      "information\n",
      "brought\n",
      "by\n",
      "the\n",
      "search\n",
      "engine\n",
      "just\n",
      "by\n",
      "UNKNOWNTOKEN\n",
      "the\n",
      "text\n",
      "of\n",
      "interest\n",
      "SENTENCEEND\n",
      "SENTENCESTART\n",
      "we\n",
      "bring\n",
      "all\n",
      "the\n",
      "UNKNOWNTOKEN\n",
      "of\n",
      "what\n",
      "the\n",
      "internet\n",
      "really\n",
      "is\n",
      "to\n",
      "the\n",
      "page\n",
      "you\n",
      "are\n",
      "reading\n",
      "SENTENCEEND\n",
      "SENTENCESTART\n",
      "UNKNOWNTOKEN\n",
      "brings\n",
      "improved\n",
      "productivity\n",
      "contextual\n",
      "collaboration\n",
      "and\n",
      "intuitive\n",
      "UNKNOWNTOKEN\n",
      "capabilities\n",
      "SENTENCEEND\n"
     ]
    }
   ],
   "source": [
    "for i in tokenized_sentences[3]:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Internet'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 74070 unique words tokens.\n"
     ]
    }
   ],
   "source": [
    "# Count the word frequencies\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print \"Found %d unique words tokens.\" % len(word_freq.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#word_freq = sorted(word_freq.items(), key=lambda i: i[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vocabulary size 5000.\n",
      "The least frequent word in our vocabulary is 'ar' and appeared 25 times.\n"
     ]
    }
   ],
   "source": [
    "# Get the most common words and build index_to_word and word_to_index vectors\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    "\n",
    "print \"Using vocabulary size %d.\" % vocabulary_size\n",
    "print \"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1])\n",
    "\n",
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'SENTENCEEND': 1, u'and': 0}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{word_to_index.keys()[word_to_index.values().index(k)]: k for k in sorted(word_to_index.values())[:2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
