{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import pickle\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import itertools\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import unicodedata\n",
    "import sys\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mypath = '/Users/Lucy/Google Drive/MSDS/2016Fall/DSGA1006/Data'\n",
    "f = open(mypath + '/lstm-rnn/lstm_data.pickle','rb')\n",
    "X_train, X_test, y_train, y_test = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_description = X_train.clean_description.copy()\n",
    "test_description = X_test.clean_description.copy()\n",
    "#train_description = [word for word in train_description if word not in stopwords.words('english')]\n",
    "#test_description = [word for word in test_description if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 10000\n",
    "unknown_token = \"UNKNOWNTOKEN\"\n",
    "sentence_start_token = \"SENTENCESTART\"\n",
    "sentence_end_token = \"SENTENCEEND\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split full descriptions into sentences\n",
    "sentences = itertools.chain(*[[nltk.sent_tokenize(x.decode('utf-8').lower())] for x in train_description])\n",
    "mod_sentences = []\n",
    "for desc in sentences:\n",
    "    full_desc = \"\"\n",
    "    for x in desc:\n",
    "        s = \"%s %s %s \" % (sentence_start_token, x, sentence_end_token)\n",
    "        full_desc = full_desc + s\n",
    "    mod_sentences.append(full_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strip_punctuation(text):\n",
    "    \"\"\"\n",
    "    >>> strip_punctuation(u'something')\n",
    "    u'something'\n",
    "\n",
    "    >>> strip_punctuation(u'something.,:else really')\n",
    "    u'somethingelse really'\n",
    "    \"\"\"\n",
    "    punctutation_cats = set(['Pc', 'Pd', 'Ps', 'Pe', 'Pi', 'Pf', 'Po'])\n",
    "    return ''.join(x for x in text\n",
    "                   if unicodedata.category(x) not in punctutation_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mod_sentences = map(strip_punctuation, mod_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCESTART cosmocom inc operates an online contact center platform and provides private and public cloud contact center solutions SENTENCEEND \n",
      "\n",
      "SENTENCESTART weddinglovely is a collection of wedding tools and directories for promoting small business local and independent wedding vendors SENTENCEEND \n",
      "\n",
      "SENTENCESTART arctic empire is home to a small team of creative tech weirdos who have come together to create awesome digital experiences SENTENCEEND \n",
      "\n",
      "SENTENCESTART vegaster SENTENCEEND SENTENCESTART amazingly simple experiences SENTENCEEND \n",
      "\n",
      "SENTENCESTART trade execution and custody services SENTENCEEND \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in mod_sentences[:5]:\n",
    "    print i + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in mod_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'apps'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 51110 unique words tokens.\n"
     ]
    }
   ],
   "source": [
    "# Count the word frequencies\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print \"Found %d unique words tokens.\" % len(word_freq.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#word_freq = sorted(word_freq.items(), key=lambda i: i[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vocabulary size 10000.\n",
      "The least frequent word in our vocabulary is 'melting' and appeared 2 times.\n"
     ]
    }
   ],
   "source": [
    "# Get the most common words and build index_to_word and word_to_index vectors\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    "\n",
    "print \"Using vocabulary size %d.\" % vocabulary_size\n",
    "print \"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1])\n",
    "\n",
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'SENTENCEEND': 1, u'and': 0}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{word_to_index.keys()[word_to_index.values().index(k)]: k for k in sorted(word_to_index.values())[:2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
