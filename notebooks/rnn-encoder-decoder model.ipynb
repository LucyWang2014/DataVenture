{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.core import RepeatVector\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD, Adam, Adadelta\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mypath = '/Users/Lucy/Google Drive/MSDS/2016Fall/DSGA1006/Data/rnn-encoder-decoder'\n",
    "f = open(mypath + '/crunchbase.train.10000.pickle','rb')\n",
    "train_x, train_y = pickle.load(f)\n",
    "f.close()\n",
    "f = open(mypath + '/crunchbase.test.10000.pickle','rb')\n",
    "test_x, test_y = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pad(x, length = 100):\n",
    "\t# truncate and pad input sequences\n",
    "\tmax_desc_length = length\n",
    "\ttrain_x = sequence.pad_sequences(x, maxlen=max_desc_length)\n",
    "\n",
    "\treturn train_x\n",
    "\n",
    "def transform_data(x, top_words = 10000):\n",
    "\tdim = x.shape\n",
    "\ttransformed = np.zeros((dim[0],dim[1],top_words+1))\n",
    "\tfor i,seq in enumerate(x):\n",
    "\t\tfor j,word in enumerate(seq):\n",
    "\t\t\ttransformed[i][j][word] = 1\n",
    "\n",
    "\treturn transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = pad(train_x, 100)\n",
    "test_x = pad(test_x, 100)\n",
    "train_y = pad(train_y, 15)\n",
    "test_y = pad(test_y, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    2,    1,   60,   10,\n",
       "          7,  132,    8,  140, 2042, 2248,  893,   66,   64,    4,   40,\n",
       "        310,    4,  219,   33,    3,    2,   57,    9,  828,    4,   42,\n",
       "          9,    1, 7175,    1,   10, 7481,   12,  437,   23,  103,   26,\n",
       "         68,  108,    1,    1, 4518,    1,    1,  168, 6793,    4,    1,\n",
       "        719], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dim = train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformed = np.zeros((dim[0],dim[1],10001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,seq in enumerate(train_x):\n",
    "    for j,word in enumerate(seq):\n",
    "        transformed[i][j][word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0., ...,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed[0][99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 5000\n",
    "unknown_token = \"UNKNOWNTOKEN\"\n",
    "sentence_start_token = \"SENTENCESTART\"\n",
    "sentence_end_token = \"SENTENCEEND\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split full descriptions into sentences\n",
    "sentences = itertools.chain(*[[nltk.sent_tokenize(x.decode('utf-8').lower())] for x in train_description])\n",
    "mod_sentences = []\n",
    "for desc in sentences:\n",
    "    full_desc = \"\"\n",
    "    for x in desc:\n",
    "        s = \"%s %s %s \" % (sentence_start_token, x, sentence_end_token)\n",
    "        full_desc = full_desc + s\n",
    "    mod_sentences.append(full_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'SENTENCESTART a smart glove that helps workers in logistics and manufacturing work more efficiently.a smart glove that helps workers in logistics and manufacturing work more efficiently. SENTENCEEND ',\n",
       " u'SENTENCESTART passlogix provides fast and secure access to enterprise resources with their flagship product; the v-go access accelerator suite.passlogix provides fast and secure access to enterprise resources with their flagship product; the v-go access accelerator suite. SENTENCEEND ']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_sentences[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strip_punctuation(text):\n",
    "    \"\"\"\n",
    "    >>> strip_punctuation(u'something')\n",
    "    u'something'\n",
    "\n",
    "    >>> strip_punctuation(u'something.,:else really')\n",
    "    u'somethingelse really'\n",
    "    \"\"\"\n",
    "    punctutation_cats = set(['Pc', 'Pd', 'Ps', 'Pe', 'Pi', 'Pf', 'Po'])\n",
    "    return ''.join(x for x in text\n",
    "                   if unicodedata.category(x) not in punctutation_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mod_sentences = map(strip_punctuation, mod_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCESTART a smart glove that helps workers in logistics and manufacturing work more efficientlya smart glove that helps workers in logistics and manufacturing work more efficiently SENTENCEEND \n",
      "\n",
      "SENTENCESTART passlogix provides fast and secure access to enterprise resources with their flagship product the vgo access accelerator suitepasslogix provides fast and secure access to enterprise resources with their flagship product the vgo access accelerator suite SENTENCEEND \n",
      "\n",
      "SENTENCESTART were on a mission to provide reliable solar energy for schools hospitals and water projects around the worldwere on a mission to provide reliable solar energy for schools hospitals and water projects around the world SENTENCEEND \n",
      "\n",
      "SENTENCESTART haus bioceuticals is a specialty pharmaceutical company developing clinically active botanical extractshaus bioceuticals is a specialty pharmaceutical company developing clinically active botanical extracts SENTENCEEND \n",
      "\n",
      "SENTENCESTART kivera foresaw the emergence of the wireless location services marketkivera foresaw the emergence of the wireless location services market SENTENCEEND \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in mod_sentences[:5]:\n",
    "    print i + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in mod_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCESTART\n",
      "haus\n",
      "bioceuticals\n",
      "is\n",
      "a\n",
      "specialty\n",
      "pharmaceutical\n",
      "company\n",
      "developing\n",
      "clinically\n",
      "active\n",
      "botanical\n",
      "extractshaus\n",
      "bioceuticals\n",
      "is\n",
      "a\n",
      "specialty\n",
      "pharmaceutical\n",
      "company\n",
      "developing\n",
      "clinically\n",
      "active\n",
      "botanical\n",
      "extracts\n",
      "SENTENCEEND\n"
     ]
    }
   ],
   "source": [
    "for i in tokenized_sentences[3]:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Lydia's iPhone and Android apps enable you to pay anyone very easily and for free.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 109713 unique words tokens.\n"
     ]
    }
   ],
   "source": [
    "# Count the word frequencies\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print \"Found %d unique words tokens.\" % len(word_freq.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#word_freq = sorted(word_freq.items(), key=lambda i: i[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vocabulary size 5000.\n",
      "The least frequent word in our vocabulary is 'hold' and appeared 16 times.\n"
     ]
    }
   ],
   "source": [
    "# Get the most common words and build index_to_word and word_to_index vectors\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    "\n",
    "print \"Using vocabulary size %d.\" % vocabulary_size\n",
    "print \"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1])\n",
    "\n",
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'SENTENCEEND': 1, u'and': 0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{word_to_index.keys()[word_to_index.values().index(k)]: k for k in sorted(word_to_index.values())[:2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50691,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tokenized_sentences).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
